{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9365ee1c",
   "metadata": {},
   "source": [
    "<a href=\"https://www.undp.org/\" ><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9f/UNDP_logo.svg\" style=\"float:right; max-width: 40px; display: inline\" alt=\"UNDP\"/></a> \n",
    "\n",
    "# *CoastPred*: example at *Test-site name*\n",
    "\n",
    "This software is described more in details in the README. \n",
    "\n",
    "It enables the users to extract time-series of shoreline change over the last 30+ years at their site of interest and predict its evolution in the future years.\n",
    "There are six main steps:\n",
    "1. Retrieval of the satellite images of the region of interest from Google Earth Engine\n",
    "2. Shoreline extraction at sub-pixel resolution\n",
    "3. Intersection of the shorelines with cross-shore transects\n",
    "4. Tidal correction \n",
    "5. Time series forecasting\n",
    "4. Extraction of the predicted shorelines\n",
    "\n",
    "## Initial settings\n",
    "\n",
    "Refer to the **Installation** section of the README for instructions on how to install the Python packages necessary to run the software, including Google Earth Engine Python API. If that step has been completed correctly, the following packages should be imported without any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977c4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.ion()\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from python import extract_shorelines as es\n",
    "from python import analyze_shoreline as asl\n",
    "from python import correct_tides as ct\n",
    "from python import predict as pt\n",
    "from python import reconstruct_shoreline as rs\n",
    "from python import estimate_pop as ep\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b877bd6",
   "metadata": {},
   "source": [
    "## 1. Retrieval of the images from GEE\n",
    "\n",
    "Define the region of interest (`polygon`), the date range (`dates`) and the satellite missions (`sat_list`) from which you wish to retrieve the satellite images. The images will be cropped on the Google Earth Engine server and only the region of interest will be downloaded as a .tif file. The files will stored in the directory defined in `filepath`. \n",
    "\n",
    "Make sure the area of your ROI is smaller than 100 km2 (if larger split it into smaller ROIs).\n",
    "\n",
    "The function `SDS_download.check_images_available(inputs)` will print the number of images available for your inputs. The Landsat images are divided in Tier 1 and Tier 2, only Tier 1 images can be used for time-series analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b8f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region of interest (longitude, latitude) - replace the coordinates with your test-site\n",
    "polygon = [[[0.9840004042206285,5.868659802766709], \n",
    "            [1.002491492775628,5.868997660770736],\n",
    "            [1.003566497639035,5.912882741342837],\n",
    "            [0.9835488890625466,5.913424406492211],\n",
    "            [0.9840004042206285,5.868659802766709]]]\n",
    "\n",
    "# it's recommended to convert the polygon to the smallest rectangle (sides parallel to coordinate axes)       \n",
    "polygon = SDS_tools.smallest_rectangle(polygon)\n",
    "# date range\n",
    "dates = ['2010-01-01', '2022-01-01']\n",
    "# satellite missions\n",
    "sat_list = ['S2','L5','L7','L8','L9']\n",
    "# name of the site\n",
    "sitename = 'GHANA'\n",
    "# directory where the data will be stored\n",
    "filepath = os.path.join(os.getcwd(), 'data')\n",
    "# put all the inputs into a dictionnary\n",
    "inputs = {'polygon': polygon, 'dates': dates, 'sat_list': sat_list, 'sitename': sitename, 'filepath':filepath}\n",
    "\n",
    "# before downloading the images, check how many images are available for your inputs\n",
    "#SDS_download.check_images_available(inputs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85581510",
   "metadata": {},
   "source": [
    "Check whether it is necessary to redefine the metadata or not, i.e. check **if you have already retrieved the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ed4061",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redefine =  False\n"
     ]
    }
   ],
   "source": [
    "# check if there is a 'references' dir containing predefined metadata\n",
    "is_ref = os.path.isdir(os.path.join(filepath,sitename,'references'))\n",
    "    \n",
    "# redefine if files do not exist yet\n",
    "redefine = not(os.path.exists(os.path.join(filepath,sitename,'%s_metadata.pkl'%(sitename))))\n",
    "    \n",
    "# actually we do not redefine the metadata if there are the references data\n",
    "if (redefine and is_ref):\n",
    "    redefine = False\n",
    "\n",
    "# or the user can choose to crash existing files and redefine metadata\n",
    "# or even force to continue with existing files (/!\\ may lead to errors /!\\)\n",
    "# redefine = False ##### TO REMOVE ###########\n",
    "\n",
    "print('redefine = ',redefine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8b239",
   "metadata": {},
   "source": [
    "\n",
    "The function `SDS_download.retrieve_images(inputs)` retrives the satellite images from Google Earth Engine.\n",
    "\n",
    "By default, only Landsat Tier 1 Top-of-Atmosphere and Sentinel-2 Level-1C products are downloaded. \n",
    "\n",
    "In case you need to access Tier 2 images for qualitative analysis, you need to set `inputs['include_T2'] = True` before calling `retrieve_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88cb8bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if redefine:\n",
    "    # this function retrieves the satellite images from Google Earth Engine \n",
    "    #inputs['include_T2'] = True\n",
    "    metadata = SDS_download.retrieve_images(inputs)\n",
    "else:\n",
    "    # if images already retrieved, just load the metadata file by only running the function below\n",
    "    metadata = SDS_download.get_metadata(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a3da1",
   "metadata": {},
   "source": [
    "##### 2. Shoreline extraction\n",
    "\n",
    "This section maps the position of the shoreline on the satellite images. The user can define the cloud threhold (`cloud_thresh`) and select the spatial reference system in which to output the coordinates of the mapped shorelines (`output_epsg`). See http://spatialreference.org/ to find the EPSG number corresponding to your local coordinate system. Make sure that your are using cartesian coordinates and not spherical coordinates (lat,lon) like WGS84. If unsure, use 3857 which is the web mercator projection (used by Google Maps).\n",
    "\n",
    "To quality control each shoreline detection and manually validate the mapped shorelines, the user has the option to set the parameter `check_detection` to **True**. \n",
    "To adjust the position of each shoreline by modifying the threshold defining the sand/water interface you can set `adjust_detection` to **True**. \n",
    "Finally, to save a figure for each mapped shoreline as a .jpg in the folder */jpg_files/detection* set `save_figure` to **True**. \n",
    "\n",
    "The other parameters are for advanced users only and are described in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ce82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = { \n",
    "    # general parameters:\n",
    "    'cloud_thresh': 0.5,        # threshold on maximum cloud cover\n",
    "    'output_epsg': 3857,        # epsg code of spatial reference system desired for the output  \n",
    "    'pan_off': True, \n",
    "    # quality control:\n",
    "    'check_detection': True,    # if True, shows each shoreline detection to the user for validation\n",
    "    'adjust_detection': False,  # if True, allows user to adjust the postion of each shoreline by changing the threhold\n",
    "    'save_figure': True,        # if True, saves a figure showing the mapped shoreline for each image\n",
    "    # [ONLY FOR ADVANCED USERS] shoreline detection parameters:\n",
    "    'min_beach_area': 500,     # minimum area (in metres^2) for an object to be labelled as a beach\n",
    "    'buffer_size': 150,         # radius (in metres) for buffer around sandy pixels considered in the shoreline detection\n",
    "    'min_length_sl': 2300,      # minimum length (in metres) of shoreline perimeter to be valid\n",
    "    'cloud_mask_issue': False,  # switch this parameter to True if sand pixels are masked (in black) on many images  \n",
    "    'sand_color': 'default',    # 'default', 'dark' (for grey/black sand beaches) or 'bright' (for white sand beaches)\n",
    "    # add the inputs defined previously\n",
    "    'inputs': inputs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530ae4c",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Save .jpg of the satellite images \n",
    "Saves .jpg files of the preprocessed satellite images (cloud masking + pansharpening/down-sampling) under *./data/sitename/jpeg_files\\preprocessed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdcb1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options available only if the metadata and the files have to be redefined\n",
    "if redefine:\n",
    "    save_jpg = True    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98753a75",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Digitize a reference shoreline\n",
    "Creates a reference shoreline which helps to identify outliers and false detections. The reference shoreline is manually digitised by the user on one of the images. The parameter `max_dist_ref` defines the maximum distance from the reference shoreline (in metres) at which a valid detected shoreline can be. If you think that the default value of 100 m will not capture the full shoreline variability of your site, increase this value to an appropriate distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154d959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference shoreline already exists and was loaded\n"
     ]
    }
   ],
   "source": [
    "# [optional] create a reference shoreline (helps to identify outliers and false detections)\n",
    "ref_shoreline = True\n",
    "if ref_shoreline:\n",
    "    %matplotlib qt\n",
    "    settings['reference_shoreline'] = SDS_preprocess.get_reference_sl(metadata, settings)\n",
    "    # set the max distance (in meters) allowed from the reference shoreline for a detected shoreline to be valid\n",
    "    settings['max_dist_ref'] = 200  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa304ad6",
   "metadata": {},
   "source": [
    "### Batch shoreline detection\n",
    "Extracts the 2D shorelines from the images in the spatial reference system specified by the user in `'output_epsg'`. The mapped shorelines are saved into `output.pkl` (under *./data/sitename*) and `output.geojson` (to be used in a GIS software).\n",
    "\n",
    "If you see that the sand pixels on the images are not being identified, change the parameter `sand_color` from `default` to `dark` or `bright` depending on the color of your beach. \n",
    "\n",
    "The coordinates are stored in the output dictionnary together with the exact dates in UTC time, the georeferencing accuracy and the cloud cover. This function also removes duplicates and images with inaccurate georeferencing (threhsold at 10m) and makes a simple plot of the mapped shorelines. \n",
    "\n",
    "For use in GIS applications, the mapped shorelines are saved as a GEOJSON layer and shapefiles which can be easily imported into QGIS for example. You can choose to save the shorelines as a collection of lines or points (sometimes the lines are crossing over so better to use points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86af6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (redefine):\n",
    "    output = es.extract_shorelines(metadata, settings, inputs, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c4809",
   "metadata": {},
   "source": [
    "## 3. Shoreline analysis\n",
    "\n",
    "In this section we show how to compute time-series of cross-shore distance along user-defined shore-normal transects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857c10f",
   "metadata": {},
   "source": [
    "**If you have already mapped the shorelines**, just load the output file (`output.pkl`) by running the section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2634d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(redefine):\n",
    "    filepath = os.path.join(inputs['filepath'], sitename)\n",
    "    with open(os.path.join(filepath, sitename + '_output' + '.pkl'), 'rb') as f:\n",
    "        output = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c67ff",
   "metadata": {},
   "source": [
    "Now we have to **define cross-shore transects** over which to quantify the shoreline changes. Each transect is defined by two points, its **origin (always landward**) and a second point that defines its orientation (so that the transect is normal to the shorelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78e49017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 transects have been loaded\n"
     ]
    }
   ],
   "source": [
    "if (redefine):\n",
    "    # the user will interactively draw the shore-normal transects along the beach by calling:\n",
    "    transects = SDS_transects.draw_transects(output, settings)\n",
    "else:\n",
    "    # the user will load the transect coordinates (make sure the spatial reference system is the same \n",
    "    # as defined previously by the parameter *output_epsg*) from a .geojson file by calling:\n",
    "    #transects = SDS_transects.draw_transects(output, settings)\n",
    "    geojson_file = os.path.join(inputs['filepath'],sitename, '%s_transects.geojson'%(sitename))\n",
    "    transects = SDS_tools.transects_from_geojson(geojson_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e1f0c",
   "metadata": {},
   "source": [
    "`analyze_shoreline` intersects the transects with the 2D shorelines to obtain time-series of cross-shore distance.\n",
    "\n",
    "The time-series of shoreline change for each transect are saved in a .csv file in the data folder (all dates are in UTC time). \n",
    "\n",
    "It also plots the location of the transects, make sure they are in the right location with the origin always landwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a6a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-series of the shoreline change along the transects saved as:\n",
      "E:\\UNDP\\Coastline prediction tool\\to_GitHub\\coastsat_dev\\data\\GHANA\\transect_time_series.csv\n"
     ]
    }
   ],
   "source": [
    "# defines the along-shore distance over which to consider shoreline points to compute the median intersection (robust to outliers)\n",
    "settings['along_dist'] = 25 \n",
    "# compute and plot the time series\n",
    "cross_distance = asl.analyze_shoreline(output,transects,settings,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af753eb1",
   "metadata": {},
   "source": [
    "## 4. Tidal correction\n",
    "\n",
    "This section shows how to tidally-correct the time-series of shoreline change using time-series of tide level and an estimate of the beach slope.\n",
    "\n",
    "When using your own file make sure that the dates are in UTC time, as the shorelines are also in UTC, and the datum for the water levels is approx. Mean Sea Level.\n",
    "\n",
    "We assume that the beach slope at West Point is 0.1 along all transects.\n",
    "\n",
    "**Note**: if you don't have measured water levels and beach slope, it is possible to obtain an estimate of the beach slope and time-series of modelled tide levels at the time of image acquisition from the [FES2014](https://www.aviso.altimetry.fr/es/data/products/auxiliary-products/global-tide-fes/description-fes2014.html) global tide model by using the [CoastSat.slope](https://github.com/kvos/CoastSat.slope) repository (see [this publication](https://doi.org/10.1029/2020GL088365) for more details, open acess preprint [here](https://www.essoar.org/doi/10.1002/essoar.10502903.1)). Instructions on how to install the global tide model are available [here](https://github.com/kvos/CoastSat.slope/blob/master/doc/FES2014_installation.md).\n",
    "\n",
    "Apply tidal correction using a linear slope and a reference elevation to which project all the time-series of cross-shore change (to get time-series at Mean Sea Level, set `reference elevation` to 0. You also need an estimate of the beach slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfba0153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-10-06 10:11:24+00:00\n",
      "Extracting closest points: 1%2013-11-23 10:11:06+00:00\n",
      "Extracting closest points: 2%2014-04-16 10:09:27+00:00\n",
      "Extracting closest points: 4%2014-05-02 10:09:13+00:00\n",
      "Extracting closest points: 5%2014-05-18 10:09:02+00:00\n",
      "Extracting closest points: 6%2015-09-10 10:09:24+00:00\n",
      "Extracting closest points: 8%2015-09-22 10:27:10+00:00\n",
      "Extracting closest points: 9%2015-09-26 10:09:31+00:00\n",
      "Extracting closest points: 10%2015-12-11 10:19:05+00:00\n",
      "Extracting closest points: 12%2015-12-11 10:29:44+00:00\n",
      "Extracting closest points: 13%2015-12-15 10:09:38+00:00\n",
      "Extracting closest points: 15%2015-12-21 10:30:09+00:00\n",
      "Extracting closest points: 16%2016-01-20 10:30:06+00:00\n",
      "Extracting closest points: 17%2016-03-04 10:09:24+00:00\n",
      "Extracting closest points: 19%2016-05-09 10:28:43+00:00\n",
      "Extracting closest points: 20%2016-10-16 10:21:48+00:00\n",
      "Extracting closest points: 21%2016-10-26 10:25:08+00:00\n",
      "Extracting closest points: 23%2017-11-10 10:27:51+00:00\n",
      "Extracting closest points: 24%2017-12-15 10:23:32+00:00\n",
      "Extracting closest points: 26%2017-12-25 10:27:52+00:00\n",
      "Extracting closest points: 27%2018-02-22 10:09:16+00:00\n",
      "Extracting closest points: 28%2018-02-23 10:27:31+00:00\n",
      "Extracting closest points: 30%2018-03-10 10:09:08+00:00\n",
      "Extracting closest points: 31%2018-03-10 10:22:11+00:00\n",
      "Extracting closest points: 32%2018-04-11 10:08:52+00:00\n",
      "Extracting closest points: 34%2018-04-27 10:08:43+00:00\n",
      "Extracting closest points: 35%2018-08-02 10:21:24+00:00\n",
      "Extracting closest points: 36%2018-08-27 10:18:08+00:00\n",
      "Extracting closest points: 38%2018-09-18 10:09:09+00:00\n",
      "Extracting closest points: 39%2018-09-26 10:17:04+00:00\n",
      "Extracting closest points: 41%2018-10-04 10:09:16+00:00\n",
      "Extracting closest points: 42%2018-10-26 10:22:08+00:00\n",
      "Extracting closest points: 43%2018-10-31 10:19:11+00:00\n",
      "Extracting closest points: 45%2019-01-08 10:09:22+00:00\n",
      "Extracting closest points: 46%2019-01-14 10:29:21+00:00\n",
      "Extracting closest points: 47%2019-01-24 10:09:18+00:00\n",
      "Extracting closest points: 49%2019-01-24 10:29:22+00:00\n",
      "Extracting closest points: 50%2019-01-29 10:29:25+00:00\n",
      "Extracting closest points: 52%2019-02-03 10:29:22+00:00\n",
      "Extracting closest points: 53%2019-02-23 10:29:21+00:00\n",
      "Extracting closest points: 54%2019-03-20 10:29:25+00:00\n",
      "Extracting closest points: 56%2019-04-04 10:29:23+00:00\n",
      "Extracting closest points: 57%2019-04-09 10:29:29+00:00\n",
      "Extracting closest points: 58%2019-04-14 10:08:59+00:00\n",
      "Extracting closest points: 60%2019-05-14 10:29:29+00:00\n",
      "Extracting closest points: 61%2019-07-18 10:29:34+00:00\n",
      "Extracting closest points: 63%2019-08-12 10:29:29+00:00\n",
      "Extracting closest points: 64%2019-09-16 10:29:25+00:00\n",
      "Extracting closest points: 65%2019-09-26 10:29:25+00:00\n",
      "Extracting closest points: 67%2019-10-31 10:29:28+00:00\n",
      "Extracting closest points: 68%2019-11-05 10:29:25+00:00\n",
      "Extracting closest points: 69%2019-11-08 10:09:53+00:00\n",
      "Extracting closest points: 71%2019-11-15 10:29:24+00:00\n",
      "Extracting closest points: 72%2019-11-20 10:29:26+00:00\n",
      "Extracting closest points: 73%2019-11-24 10:09:50+00:00\n",
      "Extracting closest points: 75%2019-12-10 10:09:49+00:00\n",
      "Extracting closest points: 76%2019-12-10 10:29:22+00:00\n",
      "Extracting closest points: 78%2020-01-04 10:29:20+00:00\n",
      "Extracting closest points: 79%2020-01-19 10:29:19+00:00\n",
      "Extracting closest points: 80%2020-02-08 10:29:17+00:00\n",
      "Extracting closest points: 82%2020-02-23 10:29:22+00:00\n",
      "Extracting closest points: 83%2020-03-09 10:29:21+00:00\n",
      "Extracting closest points: 84%2020-03-15 10:09:22+00:00\n",
      "Extracting closest points: 86%2020-03-29 10:29:21+00:00\n",
      "Extracting closest points: 87%2020-06-03 10:09:02+00:00\n",
      "Extracting closest points: 89%2020-08-06 10:09:28+00:00\n",
      "Extracting closest points: 90%2020-08-06 10:29:31+00:00\n",
      "Extracting closest points: 91%2020-10-20 10:29:29+00:00\n",
      "Extracting closest points: 93%2020-11-10 10:09:47+00:00\n",
      "Extracting closest points: 94%2020-11-14 10:29:29+00:00\n",
      "Extracting closest points: 95%2020-12-04 10:29:24+00:00\n",
      "Extracting closest points: 97%2020-12-12 10:09:52+00:00\n",
      "Extracting closest points: 98%2020-12-19 10:29:21+00:00\n",
      "Extracting closest points: 100%Tidally-corrected time-series of the shoreline change along the transects saved as:\n",
      "E:\\UNDP\\Coastline prediction tool\\to_GitHub\\coastsat_dev\\data\\GHANA\\transect_time_series_tidally_corrected.csv\n"
     ]
    }
   ],
   "source": [
    "reference_elevation = 0.0 # elevation at which you would like the shoreline time-series to be => MLLW datum = 0 here\n",
    "beach_slope = 0.5 # replace the slope with value computed in slope module\n",
    "\n",
    "cross_distance = ct.correct_tides(cross_distance,settings,output,reference_elevation,beach_slope,estimate_slope=False,plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792e972",
   "metadata": {},
   "source": [
    "Rebuild these corrected distances as shorelines and save them as shapefiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da6a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_sl = rs.reconstruct_shoreline(cross_distance,transects,output['dates'],output,inputs,settings,len(output['dates']),save_corrections=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86766c8e",
   "metadata": {},
   "source": [
    "## 5. Computation of the generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc337f",
   "metadata": {},
   "source": [
    "The function `cross_validation` computes the **generalization error of the prediction** by splitting the images in train and test samples and comparing the predicted shorelines with shorelines hand-drawn on 5m-resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d0da711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid number of years.\n"
     ]
    }
   ],
   "source": [
    "# define the number of years to be predicted\n",
    "n_years_further = 3\n",
    "\n",
    "message, validity = pt.validate_year(n_years_further, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bec28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ivana\\miniconda3\\envs\\coastsat_sdg_ai_test2\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization error (mean mae) for the year  1  in the future:  4.770303575327556 \n",
      "\n",
      "Generalization error (mean mae) for the year  2  in the future:  25.41914184238775 \n",
      "\n",
      "Generalization error (mean mae) for the year  3  in the future:  107.27553519385322 \n",
      "\n",
      "Best parameters:  1.0\n",
      "======================================================\n",
      "Mean absolute error: \n",
      " {'1 average per year': {'year 2018': 11.69208648329934, 'year 2019': 63.4748192035013, 'year 2020': 221.5894291109285}, '1': 101.48863887034062, '2 average per year': {'year 2018': 2.6955549551496323, 'year 2019': 39.20289934800899, 'year 2020': 167.36774151823604}, '2': 71.73420503534034, '3 average per year': {'year 2018': 3.1535343169713608, 'year 2019': 36.637536170442786, 'year 2020': 147.688161030357}, '3': 64.24296588696782, '4 average per year': {'year 2018': 5.203266924501895, 'year 2019': 42.839207767710214, 'year 2020': 170.27520007458116}, '4': 74.76559895142503, '5 average per year': {'year 2018': 5.140872613739338, 'year 2019': 43.577348686217626, 'year 2020': 187.31725692699797}, '5': 80.8499443070857, '6 average per year': {'year 2018': 4.957664302968225, 'year 2019': 36.644003127971935, 'year 2020': 161.6134205200994}, '6': 69.59323695455957, '7 average per year': {'year 2018': 2.407135007414182, 'year 2019': 19.71256229123527, 'year 2020': 107.2932283308613}, '7': 44.34408727287041, '8 average per year': {'year 2018': 1.1994915238573083, 'year 2019': 10.910904741197758, 'year 2020': 61.06167552973154}, '8': 25.077751533687735, '9 average per year': {'year 2018': 1.8437998698645637, 'year 2019': 9.154722713633165, 'year 2020': 50.12064610215347}, '9': 20.922423283554316, '10 average per year': {'year 2018': 3.388379196171953, 'year 2019': 8.120104619831118, 'year 2020': 32.92302351930441}, '10': 15.148941674504105, '11 average per year': {'year 2018': 5.300992198202671, 'year 2019': 9.74064321052432, 'year 2020': 35.22995975189673}, '11': 17.09693473492821, '12 average per year': {'year 2018': 7.158967801212461, 'year 2019': 12.826920580538042, 'year 2020': 56.72291177899799}, '12': 26.116947407519532, '13 average per year': {'year 2018': 6.838693466551309, 'year 2019': 10.996854601126921, 'year 2020': 41.39365716810355}, '13': 20.12654105462656, '14 average per year': {'year 2018': 5.803811394681543, 'year 2019': 12.02945873148902, 'year 2020': 61.261181351696095}, '14': 26.976192897685344}\n"
     ]
    }
   ],
   "source": [
    "# model could be: Holt, ARIMA or SARIMAX\n",
    "model = 'Holt'\n",
    "\n",
    "compute_error = True\n",
    "if compute_error:\n",
    "    best_param, rmse, mae = pt.model_evaluation(cross_distance, n_years_further, metadata, output, settings, validity, model=model, smooth_data=False, smooth_coef=30, best_param=True, seasonality=2, MNDWI=False)\n",
    "    print(\"Best parameters: \",best_param)\n",
    "    print('======================================================')\n",
    "    print(\"Mean absolute error: \\n\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4d846",
   "metadata": {},
   "source": [
    "## 6. Time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14497bac",
   "metadata": {},
   "source": [
    "The function `predict` forecasts the evolution of the shoreline for the next `n_months_further` months. The default predictive model is a **Holt's Linear Trend** model `'Holt'` but you can choose to use an **AutoRegressive Integrated Moving Average model** `'ARIMA'` or an **Seasonal AutoRegressive Integrated Moving Average with eXogenous factors** `'SARIMAX'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a20eac3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_series_pred, dates_pred_m = pt.predict(cross_distance,output,inputs,settings,n_years_further,validity,model=model,param=None,smooth_data=True,smooth_coef=1,seasonality=2,plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2d5c6",
   "metadata": {},
   "source": [
    "## 7. Extraction of the new shorelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0fc7e",
   "metadata": {},
   "source": [
    "Reconstruction of the new shoreline. This new shoreline will be plotted and saved as geojson and shape files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93327886",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_sl = rs.reconstruct_shoreline(time_series_pred,transects,dates_pred_m,output,inputs,settings,n_years_further)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
